<div>
	<h1>Introduction</h1>
	<p>Don’t Decay the Learning Rate, Increase the Batch Size by Smith et al. offers a new technique for optimizing neural networks with stochastic gradient descent. It is common practice in machine learning to decrease the learning rate parameter in a neural network as epochs progress. Smith et al. show, through mathematical analysis, that the reason that this technique works is similar to the process of simulated annealing. Stochastic gradient descent produces “fluctuations” in the weights of the network, with higher learning rates leading to more severe fluctuations. Decreasing the learning rate over time decreases the size of the fluctuations as well. These fluctuations help the optimization process escape from local minima early in the training procedure, but the decreasing fluctuation size ensures that the network can settle into a minimum before the training ends, which is exactly how simulated annealing works.
Smith et al. suggest an alternative way of optimizing stochastic gradient descent based on this analysis. Since decreasing the learning rate works by decreasing the size of fluctuations, other methods of decreasing fluctuation size should also be effective. One way to decrease the size of fluctuations, they show, is to increase the size of batches over time. In fact, in most circumstances, a decrease in the learning rate by a certain factor is equivalent to multiplying the batch size by the same factor, in terms of the effect on fluctuation size. Therefore, a schedule designed for decaying learning rate can be easily transformed (by simply taking the inverse) to be directly usable as a schedule for increasing batch size, and the results should be the same.
This hypothesis is confirmed in the experiments in Smith et al.’s paper, at least for the architectures and datasets they used. They use large ResNet architectures with the CIFAR-10 and ImageNet databases, and find almost identical results when using a decreasing learning rate and increasing batch size.
Increasing the batch size is a preferable alternative to decreasing the learning rate because it leads to better performance. Increasing the batch size does not involve changing the parameters of the network, so using an increasing batch size instead of a decreasing learning rate involves significantly fewer parameter updates. Additionally, using larger batch sizes makes the training process more parallelizable.
The goal of this project is to confirm their results for simpler architectures and smaller datasets. In particular, I will make use of several different small architectures and the MNIST database of handwritten digits.
</p>
</div>
